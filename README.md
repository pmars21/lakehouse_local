# ðŸ—ï¸ Data Lakehouse - Web Security Analytics

## ðŸ“‹ Resumen del Proyecto

Sistema de anÃ¡lisis de seguridad web implementado mediante una arquitectura **Data Lakehouse** de tres capas utilizando **ClickHouse** como motor analÃ­tico columnar y **MongoDB** como sistema operacional. El proyecto procesa logs de acceso web, informaciÃ³n de usuarios y datos de reputaciÃ³n de IPs para detectar amenazas, analizar comportamiento de usuarios y generar mÃ©tricas de negocio en tiempo real.

### ðŸŽ¯ Objetivo

Implementar un pipeline de datos completo que permita:
- Ingestar datos desde mÃºltiples fuentes (CSV, JSON)
- Almacenar datos raw sin transformaciÃ³n (Bronze)
- Enriquecer y limpiar datos mediante JOINs (Silver)
- Generar KPIs ejecutivos y vistas materializadas (Gold)
- Facilitar anÃ¡lisis de seguridad, rendimiento y comportamiento de usuarios

---

## ðŸ›ï¸ Arquitectura de Tres Capas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FUENTES DE DATOS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  logs_web.csv   â”‚   users.json     â”‚ ip_reputation.json â”‚
â”‚   (Archivos)    â”‚   (MongoDB)      â”‚    (MongoDB)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                    â”‚
         â–¼                 â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CAPA BRONZE (Raw Data - ClickHouse)              â”‚
â”‚  â€¢ logs_web: Eventos HTTP sin procesar                  â”‚
â”‚  â€¢ users: InformaciÃ³n de usuarios                       â”‚
â”‚  â€¢ ip_reputation: ReputaciÃ³n de direcciones IP          â”‚
â”‚  â€¢ Todos los campos como String para flexibilidad       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CAPA SILVER (Clean & Enriched - ClickHouse)        â”‚
â”‚  â€¢ enriched_events: Tabla Ãºnica con JOINs               â”‚
â”‚  â€¢ Tipado correcto (DateTime, Int32, Bool)              â”‚
â”‚  â€¢ Enriquecimiento: logs + usuarios + reputaciÃ³n IP     â”‚
â”‚  â€¢ Limpieza de nulos y valores por defecto              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CAPA GOLD (Analytics & KPIs - ClickHouse)         â”‚
â”‚  â€¢ 12 Vistas Materializadas pre-agregadas               â”‚
â”‚  â€¢ Seguridad: Alertas, IPs maliciosas, usuarios en riesgoâ”‚
â”‚  â€¢ Rendimiento: SLA, latencias, health checks           â”‚
â”‚  â€¢ Usuarios: SegmentaciÃ³n, geografÃ­a, journeys          â”‚
â”‚  â€¢ Business Intelligence: KPIs ejecutivos, tendencias   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ—‚ï¸ Estructura del Proyecto

```
practica-final/
â”‚
â”œâ”€â”€ data/                         # ðŸ“ Datos de entrada
â”‚   â”œâ”€â”€ logs_web.csv              # Logs de acceso HTTP
â”‚   â”œâ”€â”€ users.json                # InformaciÃ³n de usuarios
â”‚   â””â”€â”€ ip_reputation.json        # ReputaciÃ³n de IPs (amenazas)
â”‚
â”œâ”€â”€ config.json                    #  Credenciales ClickHouse (MODIFICAR ESTE ARCHIVO CON LOS DATOS PROPIOS PARA PODER EJECUTAR EL PROYECTO)
â”œâ”€â”€ config.py                      #  ConfiguraciÃ³n general (rutas, URIs) (MODIFICAR ESTE ARCHIVO CON LOS DATOS PROPIOS PARA PODER EJECUTAR EL PROYECTO)
â”‚
â”œâ”€â”€ lakehouseConfig.py            #  DDL: CreaciÃ³n de estructura del Lakehouse
â”œâ”€â”€ mongo.py                      #  Carga de JSONs a MongoDB
â”œâ”€â”€ bronze_layer.py               #  Ingesta a capa Bronze (Raw)
â”œâ”€â”€ silver_layer.py               #  TransformaciÃ³n a capa Silver (Clean)
â”œâ”€â”€ gold_layer.py                 #  Agregaciones a capa Gold (KPIs)
â”‚
â”œâ”€â”€ main.py                        #  Orquestador principal (ejecuta todo)
â”œâ”€â”€ .gitignore                     # Ignora archivos sensibles
â””â”€â”€ README.md                      # ðŸ“– Esta documentaciÃ³n
```

---

## ðŸš€ Despliegue del Proyecto

### **Prerrequisitos**

| TecnologÃ­a | VersiÃ³n MÃ­nima | PropÃ³sito |
|------------|----------------|-----------|
| **Python** | 3.8+ | Lenguaje principal |
| **MongoDB** | 4.4+ | Base de datos operacional (staging) |
| **ClickHouse** | 22.0+ | Base de datos columnar OLAP |
| **pip** | Latest | Gestor de paquetes Python |

### **1. InstalaciÃ³n de Dependencias Python**

```bash
pip install pandas pymongo clickhouse-connect
```

**LibrerÃ­as utilizadas:**
- `pandas`: Lectura y manipulaciÃ³n de CSV
- `pymongo`: Driver para conectar con MongoDB
- `clickhouse-connect`: Conector oficial de ClickHouse para Python

### **2. ConfiguraciÃ³n de MongoDB**

**Paso 1:** Levanta tu servicio de Mongo DB y actualiza la URI en `config.py` si usas credenciales o servidor remoto:

```python
db_uri = 'mongodb://localhost:27017/'
db_name = "practica_final_mongodb"
```

### **3. ConfiguraciÃ³n de ClickHouse**

**Paso 1:** Crea un archivo `config.json` en la raÃ­z del proyecto con tus credenciales:

```json
{
    "host": "tu-host.clickhouse.cloud",
    "port": 8443,
    "username": "default",
    "password": "tu-password",
    "secure": true
}
```

**Ejemplo para ClickHouse local:**
```json
{
    "host": "localhost",
    "port": 8123,
    "username": "default",
    "password": "",
    "secure": false
}
```

**Paso 2:** Actualiza la ruta del config en `config.py`:

```python
config_file = r'C:\ruta\completa\al\config.json'
```

### **4. PreparaciÃ³n de Datos**

**Paso 1:** Coloca los archivos CSV y JSON en la carpeta `data/`:
- `logs_web.csv` (30 eventos HTTP)
- `users.json` (12 usuarios)
- `ip_reputation.json` (12 IPs con niveles de riesgo)

**Paso 2:** Actualiza la ruta de datos en `config.py`:

```python
ruta_data = r'C:\ruta\completa\a\la\carpeta\data'
```

### **5. EjecuciÃ³n del Pipeline Completo**

```bash
python main.py
```

**Output esperado:**
```
==================================================
ðŸš€ INICIANDO ORQUESTADOR DEL LAKEHOUSE
==================================================

[PASO 1/5] Cargando datos en MongoDB...
âœ… Insertados 12 documentos en colecciÃ³n 'users'.
âœ… Insertados 12 documentos en colecciÃ³n 'ip_reputation'.

[PASO 2/5] Creando estructura del Lakehouse...
âœ… Base de datos 'bronze' lista.
âœ… Base de datos 'silver' lista.
âœ… Base de datos 'gold' lista.

[PASO 3/5] Ingestando Capa BRONZE...
âœ… [logs_web] Ingestados 30 registros en Bronze.
âœ… [users] Ingestados 12 usuarios desde Mongo.
âœ… [ip_reputation] Ingestadas 12 IPs desde Mongo.

[PASO 4/5] Procesando Capa SILVER...
âœ… Tabla 'silver.enriched_events' verificada.
âœ… Registros generados: 27

[PASO 5/5] Ejecutando Capa GOLD...
âœ… CAPA GOLD CREADA EXITOSAMENTE
ðŸ“ˆ 12 vistas materializadas creadas

==================================================
âœ… EJECUCIÃ“N COMPLETADA CON Ã‰XITO
==================================================
```

### **6. EjecuciÃ³n Modular (Opcional)**

Puedes ejecutar cada capa por separado para debugging:

```bash
# Solo carga a MongoDB
python mongo.py

# Solo crea estructura DDL
python lakehouseConfig.py

# Solo ingesta Bronze
python bronze_layer.py

# Solo procesa Silver
python silver_layer.py

# Solo genera Gold
python gold_layer.py
```

---

## ðŸ“„ ExplicaciÃ³n de Scripts

### **1. `config.py` - ConfiguraciÃ³n Centralizada**

**PropÃ³sito:** Archivo de configuraciÃ³n que centraliza todas las rutas y credenciales del proyecto.

**Contenido:**
```python
# MongoDB
db_name = "practica_final_mongodb"
db_uri = 'mongodb://localhost:27017/'

# Rutas locales
ruta_data = r'C:\Users\...\data'
config_file = r'C:\Users\...\config.json'
```

**Por quÃ© existe:** Facilita el cambio de rutas y credenciales sin modificar el cÃ³digo principal. Un Ãºnico punto de actualizaciÃ³n para todo el proyecto.

---

### **2. `lakehouseConfig.py` - Estructura DDL del Lakehouse**

**PropÃ³sito:** Define la arquitectura de tres capas en ClickHouse mediante DDL (Data Definition Language).

**Funciones principales:**

#### `get_client()`
```python
def get_client():
    with open(conf.config_file, 'r') as file:
        config = json.load(file)
    client = clickhouse_connect.get_client(
        host=config["host"],
        port=config["port"],
        username=config["username"],
        password=config["password"],
        secure=config["secure"]
    )
    return client
```
**QuÃ© hace:** Lee las credenciales del `config.json` y devuelve un cliente conectado a ClickHouse que otros mÃ³dulos reutilizan.

#### `setup_lakehouse()`
**QuÃ© hace:**
1. **Crea 3 bases de datos:** `bronze`, `silver`, `gold`
2. **Crea tablas en Bronze:**
   - `bronze.logs_web`: 11 columnas (event_id, event_ts, user_id, ip_address, http_method, url_path, status_code, bytes_sent, response_time_ms, user_agent, is_suspicious)
   - `bronze.users`: 8 columnas (_id, username, email, role, country, created_at, is_premium, risk_score)
   - `bronze.ip_reputation`: 5 columnas (ip, source, risk_level, threat_type, last_seen)

**DecisiÃ³n de diseÃ±o:** Todas las columnas en Bronze son `String` para maximizar flexibilidad en la ingesta. No queremos que falle por un formato inesperado. La conversiÃ³n de tipos se hace despuÃ©s en Silver.

**Motor usado:** `MergeTree()` - Motor columnar optimizado de ClickHouse para OLAP.

---

### **3. `mongo.py` - Carga de Datos Operacionales**

**PropÃ³sito:** Ingesta los archivos JSON (`users.json`, `ip_reputation.json`) a MongoDB como paso intermedio.

**Funciones principales:**

#### `create_mongo_connection()`
```python
def create_mongo_connection():
    client = MongoClient(conf.db_uri)
    db = client[conf.db_name]
    return client, db
```
**QuÃ© hace:** Establece conexiÃ³n con MongoDB y devuelve el cliente y la base de datos para trabajar con colecciones.

#### `load_data_to_mongo()`
**Proceso paso a paso:**

1. **Lee `users.json` del disco:**
   ```python
   with open(path_users, 'r', encoding='utf-8') as f:
       users_data = json.load(f)
   ```

2. **Limpia la colecciÃ³n anterior (idempotencia):**
   ```python
   db.users.drop()
   ```
   Esto permite re-ejecutar el script sin duplicar datos.

3. **Inserta todos los documentos:**
   ```python
   db.users.insert_many(users_data)
   ```

4. **Repite el mismo proceso para `ip_reputation.json`**

**Por quÃ© MongoDB:** Aunque ClickHouse podrÃ­a leer JSON directamente, MongoDB sirve como "staging area" operacional. En un sistema real, estos datos vendrÃ­an de sistemas transaccionales que usan MongoDB.

---

### **4. `bronze_layer.py` - Ingesta a Capa Raw**

**PropÃ³sito:** Ingesta datos desde mÃºltiples fuentes (CSV y MongoDB) hacia la capa Bronze de ClickHouse sin transformaciones.

**FunciÃ³n principal:** `ingest_bronze()`

**Proceso de ingesta:**

#### **A. Logs Web (CSV â†’ ClickHouse)**
```python
df_logs = pd.read_csv(path_logs_csv, dtype=str)
df_logs = df_logs.fillna('')
ch_client.insert_df('bronze.logs_web', df_logs)
```

**QuÃ© hace:**
1. Lee el CSV completo con pandas
2. Fuerza todos los campos a `String` con `dtype=str`
3. Rellena valores nulos con cadena vacÃ­a
4. Inserta el DataFrame directamente en ClickHouse usando `insert_df()`

**Por quÃ© `dtype=str`:** Evita problemas de tipado. Si un campo numÃ©rico tiene un valor "N/A" en el CSV, pandas no falla porque lo trata como string.

#### **B. Usuarios (MongoDB â†’ ClickHouse)**
```python
cursor_users = mongo_db.users.find({})
users_list = list(cursor_users)

for doc in users_list:
    row = [
        str(doc.get('_id', '')),
        str(doc.get('username', '')),
        ...
    ]
    data_to_insert.append(row)

ch_client.insert('bronze.users', data_to_insert, column_names=column_names)
```

**QuÃ© hace:**
1. Recupera todos los documentos de la colecciÃ³n `users`
2. Convierte cada documento en una lista de strings
3. Inserta usando el mÃ©todo `insert()` especificando nombres de columnas

**Detalle importante:** Los booleanos de MongoDB (`is_premium: true`) se convierten a strings (`"True"`) para coincidir con la definiciÃ³n de Bronze.

#### **C. IP Reputation (MongoDB â†’ ClickHouse)**
Mismo proceso que usuarios pero para la colecciÃ³n `ip_reputation`.

**Resultado:** 3 tablas Bronze pobladas con datos raw sin transformaciones.

---

### **5. `silver_layer.py` - TransformaciÃ³n y Enriquecimiento**

**PropÃ³sito:** Crear una tabla Ãºnica enriquecida mediante JOINs, con tipos de datos correctos y valores limpios.

**FunciÃ³n principal:** `process_silver()`

**Proceso de transformaciÃ³n:**

#### **1. DefiniciÃ³n de la tabla Silver**
```sql
CREATE TABLE silver.enriched_events (
    -- Datos del Log (11 campos)
    event_id String,
    event_ts DateTime,  -- Â¡Convertido de String!
    user_id String,
    ip_address String,
    http_method String,
    url_path String,
    status_code Int32,  -- Â¡Convertido de String!
    bytes_sent Int32,
    response_time_ms Int32,
    user_agent String,
    is_suspicious UInt8,  -- Â¡Convertido a booleano!
    
    -- Datos Enriquecidos del Usuario (5 campos)
    user_name String,
    user_email String,
    user_role String,
    user_country String,
    user_is_premium Bool,
    
    -- Datos Enriquecidos de IP (3 campos)
    ip_risk_level String,
    ip_threat_type String,
    ip_source String
) ENGINE = MergeTree()
ORDER BY event_ts
```

**Total:** 19 campos (11 logs + 5 usuarios + 3 reputaciÃ³n IP)

#### **2. Query ETL con JOINs**
```sql
INSERT INTO silver.enriched_events
SELECT
    -- Logs (con conversiones de tipo)
    L.event_id,
    parseDateTimeBestEffort(L.event_ts) AS event_ts,
    L.user_id,
    L.ip_address,
    ...
    ifNull(L.bytes_sent, 0),  -- Limpieza de nulos
    
    -- JOIN con Users (LEFT JOIN porque pueden haber logs anÃ³nimos)
    if(U.username = '', 'Anonymous', U.username) as user_name,
    U.email,
    if(U.role = '', 'guest', U.role) as user_role,
    if(U.country = '', 'XX', U.country) as user_country,
    ifNull(U.is_premium, 0) as user_is_premium,
    
    -- JOIN con IP Reputation
    if(I.risk_level = '', 'unknown', I.risk_level) as ip_risk_level,
    if(I.threat_type = '', 'benign', I.threat_type) as ip_threat_type,
    I.source

FROM bronze.logs_web AS L
LEFT JOIN bronze.users AS U ON L.user_id = U._id
LEFT JOIN bronze.ip_reputation AS I ON L.ip_address = I.ip
WHERE L.user_id IS NOT NULL AND L.user_id != ''
```

**Transformaciones aplicadas:**

1. **ConversiÃ³n de tipos:**
   - `event_ts`: String â†’ DateTime usando `parseDateTimeBestEffort()`
   - `status_code`, `bytes_sent`, `response_time_ms`: String â†’ Int32
   - `is_suspicious`: String â†’ UInt8 (0 o 1)

2. **Limpieza de nulos:**
   - `ifNull(L.bytes_sent, 0)`: Convierte NULL a 0
   - Valores por defecto: 'Anonymous', 'guest', 'XX', 'unknown', 'benign'

3. **LEFT JOINs:**
   - **Â¿Por quÃ© LEFT JOIN?** Porque pueden existir:
     - Logs de usuarios no registrados (user_id vacÃ­o)
     - IPs que no estÃ¡n en nuestra base de reputaciÃ³n
   - **Con LEFT JOIN** no perdemos logs, solo quedan con valores por defecto

**Resultado:** Tabla `silver.enriched_events` con 27 registros (el filtro `WHERE user_id IS NOT NULL` elimina 3 logs anÃ³nimos de los 30 originales).

---

### **6. `gold_layer.py` - Vistas Materializadas para Analytics**

**PropÃ³sito:** Crear 12 vistas materializadas pre-agregadas que responden preguntas de negocio en milisegundos.

**FunciÃ³n principal:** `create_gold_views()`

**CategorÃ­as de KPIs (12 vistas en total):**

#### **ðŸ”’ 1. SEGURIDAD (3 vistas)**

##### **1.1. `security_daily_summary`**

**Pregunta que responde:** "Â¿CuÃ¡ntos eventos sospechosos tuvimos hoy por nivel de riesgo de IP?"

**Motor:** `SummingMergeTree()` - Suma automÃ¡ticamente valores cuando se insertan datos con la misma clave (event_date, ip_risk_level).

##### **1.2. `top_malicious_ips`**
**Pregunta:** "Â¿CuÃ¡les son las IPs mÃ¡s activas con comportamiento malicioso?"

**MÃ©tricas:**
- Conteo de requests sospechosos
- Intentos de acceso a pÃ¡ginas 404 (posible escaneo)
- URLs Ãºnicas accedidas (indica ataque distribuido)
- Promedio de tiempo de respuesta (puede indicar DoS)

##### **1.3. `user_security_alerts`**
**Pregunta:** "Â¿QuÃ© usuarios muestran seÃ±ales de compromiso?"

**Indicadores de riesgo calculados:**
```sql
calculated_risk_score = 
    countIf(is_suspicious = 1) * 10 +
    countIf(ip_risk_level = 'critical') * 20 +
    countIf(ip_risk_level = 'high') * 10 +
    uniq(ip_address) * 2
```

**Filtro:** Solo muestra usuarios con score > 50 (alertas significativas).

#### **âš¡ 2. RENDIMIENTO (3 vistas)**

##### **2.1. `endpoint_performance`**
**Pregunta:** "Â¿CuÃ¡l es el SLA y latencia de cada endpoint?"

**MÃ©tricas clave:**
```sql
quantile(0.50)(response_time_ms) AS p50_latency_ms,  -- Mediana
quantile(0.95)(response_time_ms) AS p95_latency_ms,  -- P95 (SLA tÃ­pico)
quantile(0.99)(response_time_ms) AS p99_latency_ms,  -- P99 (worst case)
(countIf(status_code < 500) * 100.0) / count() AS availability_pct
```

**Uso en producciÃ³n:** Detectar endpoints lentos o con alta tasa de error.

##### **2.2. `system_health_hourly`**
**Pregunta:** "Â¿CuÃ¡l es la salud general del sistema hora a hora?"

**Snapshot cada hora:**
- Requests totales, usuarios activos, IPs Ãºnicas
- Tasa de error global: `(countIf(status_code >= 500) * 100.0) / count()`
- Ancho de banda: `sum(bytes_sent) / 1024 / 1024` (convertido a MB)
- Eventos de seguridad

##### **2.3. `server_errors_analysis`**
**Pregunta:** "Â¿QuÃ© errores 5xx estÃ¡n ocurriendo y por quÃ©?"

**Detalles para debugging:**
- Hora exacta del primer y Ãºltimo error
- User agents afectados (para identificar si es un cliente especÃ­fico)
- Usuarios y IPs impactados

#### **ðŸ‘¥ 3. USUARIOS (3 vistas)**

##### **3.1. `user_segment_analytics`**
**Pregunta:** "Â¿CÃ³mo se comportan usuarios Premium vs Free?"

**Comparativa:**
```sql
GROUP BY analysis_date, user_is_premium, user_country, user_role
```

**MÃ©tricas:**
- Engagement: requests totales, pÃ¡ginas Ãºnicas visitadas
- Acciones interactivas: `countIf(http_method = 'POST')`
- Calidad de servicio percibida: latencia promedio, tasa de Ã©xito
- Seguridad: actividades sospechosas por segmento

##### **3.2. `geographic_activity`**
**Pregunta:** "Â¿CÃ³mo varÃ­a el uso y rendimiento por paÃ­s?"

**DistribuciÃ³n geogrÃ¡fica:**
- Volumen por paÃ­s
- Mix Premium/Free por regiÃ³n
- Performance regional (latencias P50, P95)
- Riesgos regionales (IPs de alto riesgo por paÃ­s)

##### **3.3. `user_journey_metrics`**
**Pregunta:** "Â¿CÃ³mo navegan los usuarios por la aplicaciÃ³n?"

**Path de navegaciÃ³n:**
```sql
groupArray(5)(url_path) AS navigation_path  -- Primeras 5 pÃ¡ginas
dateDiff('minute', min(event_ts), max(event_ts)) AS session_duration_minutes
```

**FricciÃ³n detectada:**
- Errores 404 (pÃ¡ginas no encontradas)
- Errores 5xx enfrentados por el usuario
- Tiempo de carga promedio

#### **ðŸ“Š 4. BUSINESS INTELLIGENCE (3 vistas)**

##### **4.1. `executive_daily_kpis`**
**Pregunta:** "Â¿CuÃ¡les son los KPIs ejecutivos del dÃ­a?"

**Audiencia:** C-level executives (CEO, CTO, CISO)

##### **4.2. `user_value_estimation`**
**Pregunta:** "Â¿QuÃ© usuarios generan mÃ¡s valor?"

**Modelo de valor (sin datos de revenue):**
```sql
estimated_value_points = 
    count() * 1.0 +                              -- Cada request = 1 punto
    countIf(http_method = 'POST') * 5.0 +        -- Cada acciÃ³n = 5 puntos
    if(user_is_premium = 1, count() * 2.0, 0)    -- Premium users 3x
```

**Uso:** Identificar usuarios high-value para retenciÃ³n o upselling.

##### **4.3. `weekly_trends`**
**Pregunta:** "Â¿CÃ³mo evolucionan las mÃ©tricas semana a semana?"

**Tendencias tracked:**
```sql
toMonday(event_ts) AS week_start  -- Agrupa por inicio de semana
```

**MÃ©tricas de crecimiento:**
- Usuarios activos semanales (WAU)
- Volumen de requests
- Tasa de Ã©xito y seguridad
- Mix de usuarios premium

**Uso:** Detectar tendencias positivas/negativas, estacionalidad.

---

### **7. `main.py` - Orquestador del Pipeline**

**PropÃ³sito:** Ejecutar el pipeline completo en el orden correcto con manejo de errores.

**Proyecto desarrollado como prÃ¡ctica final de GestiÃ³n de Almacenamiento y Big Data**